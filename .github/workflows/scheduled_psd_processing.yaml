name: Scheduled PSD Processing Demo

on:
  schedule:
    - cron: "15 * * * *"   # every hour (UTC)
  workflow_dispatch:

jobs:
  process-s3-data:
    runs-on: ubuntu-latest

    permissions:
      contents: write      # REQUIRED to commit back to repo
      id-token: write      # REQUIRED for AWS OIDC
      actions: read

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0   # needed to commit changes

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.9.18"
          cache: 'pip'
          cache-dependency-path: requirements_noise.txt

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements_noise.txt
      
      - name: Install ffmpeg
        run: |
          sudo apt-get update
          sudo apt-get install ffmpeg
      
      - uses: awalsh128/cache-apt-pkgs-action@latest
        with:
          packages: ffmpeg

      - name: Process S3 data
        if: steps.cache.outputs.cache-hit != 'true'
        run: |
          python PSD_parquet_processing_scripts/git_action_batch.py

      - name: Commit processed data to repository
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          git add data/parquet_files/

          # Only commit if there are changes
          if git diff --cached --quiet; then
            echo "No changes to commit"
            exit 0
          fi

          git commit -m "Automated data processing ($(date -u +'%Y-%m-%d %H:%M UTC'))"
          git push
